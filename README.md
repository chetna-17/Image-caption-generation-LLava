# Image-caption-generation-LLava
Image caption generating app using LLava model
<p># Image Captioning with LLAVA 1.5-7B HF

## Overview
This project focuses on generating descriptive captions for images using the **LLAVA (Large Language and Vision Assistant) 1.5-7B HF** model. The model was run in **Google Colab** using a **T4 GPU** for efficient processing. Additionally, a web application was developed using **Anvil** to provide a user-friendly interface where users can upload images and receive captions.

## About LLAVA 1.5-7B HF
The **LLAVA 1.5-7B HF** is a multi-modal model that integrates vision and language understanding. It builds upon the **Vicuna-7B** large language model and incorporates a **CLIP Vision Encoder** to process both images and text.

**Key Features:**
- **Architecture:** Vicuna-7B combined with a CLIP Vision Encoder
- **Parameters:** 7 Billion
- **Capabilities:**
  - Image Captioning
  - Visual Question Answering
  - Multi-modal Reasoning
- **Model Repository:** [llava-hf/llava-1.5-7b-hf on Hugging Face](https://huggingface.co/llava-hf/llava-1.5-7b-hf)

## Implementation Details
- **Framework:** Hugging Face **Transformers** library
- **Dataset:** **Flickr30k**, utilizing only the first caption per image
- **Quantization:** Model quantized to **4-bit** to optimize memory usage and inference speed
- **Data Processing:** Custom data collator handling `pixel_values`, `input_ids`, and `attention_mask`
- **Training Environment:** **Google Colab** with **T4 GPU**

## Web Application (Anvil)
A web application was developed using **Anvil**, enabling users to upload images and receive captions generated by the LLAVA model.

**Anvil Features:**
- Drag-and-drop interface for building web apps
- Python backend integration
- Simple deployment and sharing options

### App Functionality:
- Upload an image via the web interface
- Generate a caption using the LLAVA 1.5-7B HF model
- Display the generated caption on the web app

## How to Run the Project

### Prerequisites
- Google Account for accessing Google Colab
- Anvil account for web app access

### Running the Model
1. Open the provided Google Colab file named **usemodel**.
2. Connect the Colab environment to a GPU.
3. Run all the cells in the notebook to start the server.

### Accessing the Web Application
1. Use the provided link to open the web app.
2. Upload an image from your system.
3. The generated caption will be displayed on the page.

## Results
The LLAVA model effectively generates descriptive captions for a variety of images. The Anvil web app provides an intuitive platform for users to upload images and view the generated captions.

## Acknowledgments
- [LLAVA Team](https://huggingface.co/llava-hf)
- [Anvil](https://anvil.works/) for providing an easy-to-use platform for web development
- Hugging Face for model hosting and support

</p>
